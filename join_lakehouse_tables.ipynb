{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725f25fc-77e9-4fb1-a88c-1b5ebe9c5a12",
   "metadata": {},
   "source": [
    "# üöÄ SageMaker Lakehouse: Bridging Sales & Promotional Data for ML\n",
    "\n",
    "## üìä Overview\n",
    "\n",
    "This notebook demonstrates how to leverage Amazon SageMaker Lakehouse to create a unified dataset combining sales and promotional data from disparate storage systems. The resulting table will serve as a foundation for machine learning model development.\n",
    "\n",
    "## üõ†Ô∏è Technical Environment                                                                                                                                                    \n",
    "We're utilizing Amazon SageMaker Lakehouse, which provides:\n",
    "\n",
    "- üîÑ Unified access to data across Amazon S3 data lakes and Amazon Redshift data warehouses\n",
    "- üîç Seamless querying capabilities using Apache Iceberg-compatible tools\n",
    "- üíé Zero data replication: Query data directly where it lives, eliminating the need for copies\n",
    "\n",
    "\n",
    "## üè¢ Business Context\n",
    "\n",
    "As members of the analytics team, we need to prepare a dataset that correlates sales performance with promotional activities. Our data sources span two departments:\n",
    "\n",
    "- Central Operations üì¶: Sales data stored in Amazon S3 data lake\n",
    "- Back Office üíº: Promotional data maintained in Amazon Redshift\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "We will create a new analytical table that:\n",
    "\n",
    "1. üîó Combines sales transaction records from S3 with promotional data from Redshift\n",
    "2. üìà Calculates the number of active promotions per product category and region at the time of each sale\n",
    "3. ü§ñ Provides a cleaned and optimized dataset ready to be shared with the Machine Learning team\n",
    "    \n",
    "\n",
    "üí° SageMaker Lakehouse's unified query interface eliminates the complexity of working with multiple storage solutions, allowing us to focus on the analytical requirements rather than data access mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a28f5-3827-4c8b-8a8c-dcbc5e57cf89",
   "metadata": {},
   "source": [
    "## üîß Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb36ba-e2d0-40e3-9e0d-b59aed14c847",
   "metadata": {},
   "source": [
    "Let's initialize our development environment with the necessary libraries and Spark session configurations. The following setup enables seamless integration of Data Lake tables stored in Amazon S3 and Data Warehouse tables in Amazon Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccca3b-aee2-42a4-81db-ac3b00822a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Function to retrieve the current AWS account ID\n",
    "def get_account_id():\n",
    "    sts = boto3.client('sts')\n",
    "    return sts.get_caller_identity()['Account']\n",
    "\n",
    "# Get current AWS account ID\n",
    "account_id = get_account_id()\n",
    "\n",
    "# Define Spark session configuration dictionary with:\n",
    "# - Iceberg table format support\n",
    "# - Spark SQL extensions and catalog settings\n",
    "\n",
    "config_dict = {\n",
    "    \"--datalake-formats\": \"iceberg\",\n",
    "    \"--conf\": f\"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.rms_federated_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.rms_federated_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.rms_federated_catalog.glue.id={account_id}:federated_redshift_catalog/enterprise_operations --conf spark.sql.catalog.rms_federated_catalog.client.region=us-east-1 --conf spark.sql.catalog.rms_federated_catalog.glue.account-id={account_id} --conf spark.sql.catalog.spark_catalog.client.region=us-east-1 --conf spark.sql.catalog.spark_catalog.glue.account-id={account_id}\"\n",
    "}\n",
    "\n",
    "# Convert configuration to properly formatted JSON string\n",
    "# Replace single quotes with double quotes for JSON compatibility\n",
    "config_json = json.dumps(config_dict, indent=4).replace(\"'\", '\"')\n",
    "\n",
    "# Get IPython interface and setup the configuration\n",
    "ip = get_ipython()\n",
    "ip.run_cell_magic('configure', '-f --name project.spark.fineGrained', config_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afe34b-092d-40a7-b76d-e3d3d94163d0",
   "metadata": {},
   "source": [
    "üí° Note: Running the next cell will initialize a Glue Interactive session and may require ~1 minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae0925-9744-4824-bb76-332676f25362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b185d-f2f1-4e0e-bebf-a23952922e3e",
   "metadata": {},
   "source": [
    "## üìä Data Exploration: Glue Data Catalog (S3 Data Lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371e7a6-e9a0-4861-932f-29641cdc0787",
   "metadata": {},
   "source": [
    "Let's explore the data available in our S3-based data lake through the Glue default catalog.\n",
    "\n",
    "List all available databases in the default Glue catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d9b1c-5673-4e2a-a2db-585b05516eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show databases\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84581f29-d3a6-45d5-ab3f-a269cdbd5790",
   "metadata": {},
   "source": [
    "Show tables in the `customer_insights_db` database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411c520-bddb-4a58-a875-f40c7c2476e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show tables from customer_insights_db\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289ee87-2d33-474f-aa92-0a8f41d30af9",
   "metadata": {},
   "source": [
    "View `sales_data` table data (first 10 records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a9193-dd0e-4473-9a7c-9c7b0e477582",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"select * from customer_insights_db.sales_data limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fabb5b-37d2-41d5-ad87-586ffc50f45e",
   "metadata": {},
   "source": [
    "## üìä Data Exploration: Redshift Federated Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14194649-8d0e-4b5c-9c01-3d27a09d92e3",
   "metadata": {},
   "source": [
    "Now let's examine the data stored in the Redshift data warehouse.\n",
    "\n",
    "List all databases available in Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00d199-520e-452c-9797-714f83ec1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show databases in rms_federated_catalog\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2c9b1-db23-4286-be78-3f875cea4314",
   "metadata": {},
   "source": [
    "Show tables in Redshift's `public` schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46513372-f222-406e-9ad3-60e3e9f6aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show tables from rms_federated_catalog.public\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fe14f-ac6d-4611-a50b-037e068f6ba4",
   "metadata": {},
   "source": [
    "View `promotions` table data (first 10 records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3a8a4-7709-4b64-b124-9e1797aa5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "# Handle cold start issues with Redshift connection\n",
    "try:\n",
    "    spark.sql(f\"select * from rms_federated_catalog.public.promotions limit 1\").count()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8248c-e267-4d3f-98d3-502b98a8703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"select * from rms_federated_catalog.public.promotions limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f5b6d-42e0-4f5d-a3b3-e901cc60f61b",
   "metadata": {},
   "source": [
    "## üîÑ Preparing the Sales & Promotions Dataset for ML Modeling\n",
    "\n",
    "This section combines sales data from our data lake with promotional information from our data warehouse to create a feature-rich dataset for future Marchine Learning modeling. The resulting table will include the count of active promotions for each sale's product category and region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eef3d3-b702-4d1b-b2a3-e4d32c8753c7",
   "metadata": {},
   "source": [
    "Save the enriched dataset as a new table in our project database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b7e73-1700-4ecf-b1c1-709b7b5391cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "\n",
    "# Join sales data with promotions and calculate active promotions per sale\n",
    "final_table = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    s.*,\n",
    "    COUNT(p.promotion_id) as active_promotions\n",
    "FROM \n",
    "    customer_insights_db.sales_data s\n",
    "LEFT JOIN \n",
    "    rms_federated_catalog.public.promotions p \n",
    "    ON s.region = p.region\n",
    "    AND s.product_category = p.product_category\n",
    "    AND s.order_date BETWEEN p.start_date AND p.end_date\n",
    "GROUP BY \n",
    "    s.region,\n",
    "    s.country,\n",
    "    s.item_type,\n",
    "    s.product_category,\n",
    "    s.sales_channel,\n",
    "    s.order_priority,\n",
    "    s.order_date,\n",
    "    s.order_id,\n",
    "    s.ship_date,\n",
    "    s.units_sold,\n",
    "    s.unit_price,\n",
    "    s.unit_cost,\n",
    "    s.total_revenue,\n",
    "    s.total_cost,\n",
    "    s.total_profit\n",
    "\"\"\")\n",
    "\n",
    "# Get the project database name\n",
    "project_db = spark.sql(\"show databases\") \\\n",
    "    .filter(\"namespace != 'customer_insights_db' AND namespace != 'default'\") \\\n",
    "    .collect()[0]['namespace']\n",
    "\n",
    "# Create temporary view for final dataset\n",
    "final_table.createOrReplaceTempView(\"temp_final_table\")\n",
    "\n",
    "# Create persistent table using CTAS (Create Table As Select)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {project_db}.sales_table_enriched_w_campaigns\n",
    "    USING PARQUET\n",
    "    AS \n",
    "    SELECT * FROM temp_final_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d09d4-27e6-4a89-8bff-94fb0ea5b1e8",
   "metadata": {},
   "source": [
    "Preview the newly created enriched table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1d33f-10af-4960-a6a0-17cb1cb372f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"select * from {project_db}.sales_table_enriched_w_campaigns limit 10\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
